# -*- coding: utf-8 -*-
"""GPT_With_Tokeniser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WZnbW6BE1vk5wC12lNOkwnsAUAaSixH6
"""

import tiktoken
import torch

import time
import math
import os
import random
import inspect
import numpy as np

import torch.nn as nn
from torch.nn import functional as F

from torch.distributed import init_process_group, destroy_process_group
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# Model hyperparameters
n_embed = 448           # Size of embeddings
n_head =  8             # Number of attention heads
n_layer = 10            # Number of transformer layers
batch_size = 16         # Batch size
block_size = 1024       # Context length
dropout = 0.1           # Dropout probability
vocab_size = 100352     # Vocabulary size :: Originally = 100277, update to nice number 100352 -> divisible by 2048


# distributed data parallel
ddp = int(os.environ.get('RANK', -1)) != -1
if ddp:
    assert torch.cuda.is_available(), " look in hyperparameters section if cuda is available"
    init_process_group(backend = 'nccl')
    ddp_rank = int(os.environ['RANK'])
    ddp_local_rank = int(os.environ['LOCAL_RANK'])
    ddp_world_size = int(os.environ['WORLD_SIZE'])
    device = torch.device(f'cuda:{ddp_local_rank}')
    torch.cuda.set_device(device)
    master_process = ddp_rank == 0

else:
    ddp_rank = 0
    ddp_local_rank = 0
    ddp_world_size = 1
    master_process = True

    device = "cpu"
    if torch.cuda.is_available():
        device = "cuda"

    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = "mps"

    print(f"Using device {device}")

from google.colab import drive
drive.mount('/content/drive')

def load_tokens(filename):
    npt = np.load(filename)
    ptt = torch.tensor(npt, dtype = torch.long)
    return ptt

class DataLoader:
    def __init__(self, B, T, process_rank, num_processes, split):
        self.B = int(B)
        self.T = int(T)
        self.process_rank = process_rank
        self.num_processes = num_processes
        assert split in {'train', 'val'}

        dataset_root = '/content/drive/MyDrive/dataset_for_GPT_70M'
        shards = os.listdir(dataset_root)
        shards = [s for s in shards if split in s]
        shards = sorted(shards)
        shards = [os.path.join(dataset_root, s) for s in shards]

        self.shards = shards

        assert len(shards) > 0, f'no shards found for split {split}'

        if master_process:
            print(f'found {len(shards)} shards for split {split}')

        self.reset()

    def reset(self):
        self.current_shard = 0
        self.tokens = load_tokens(self.shards[self.current_shard])
        self.current_position = self.B * self.T * self.process_rank

    def next_batch(self):
        B, T = self.B, self.T
        buffer = self.tokens[self.current_position : self.current_position + self.B * self.T + 1]

        x = buffer[:-1].view(B, T)
        y = buffer[1:].view(B, T)

        self.current_position += B * T * self.num_processes

        if (self.current_position + B * T * self.num_processes) >= len(self.tokens):
            self.current_shard = (self.current_shard + 1) % len(self.shards)
            self.tokens = load_tokens(self.shards[self.current_shard])
            self.current_position = B * T * self.process_rank

        return x, y


# class DataLoader:
#     def __init__(self, B, T, process_rank, num_processes):
#         self.B = int(B)
#         self.T = int(T)
#         self.process_rank = process_rank
#         self.num_processes = num_processes

#         with open('input.txt', 'r') as f:
#             text = f.read()

#         enc = tiktoken.get_encoding('p50k_base')
#         tokens = enc.encode(text)
#         self.tokens = torch.tensor(tokens)

#         if master_process:
#             print(f"loaded {len(self.tokens)} tokens")
#             temp = len(self.tokens) // (B * T)
#             print(f"1 epoch = {temp} batches")

#         self.current_position = self.B * self.T * self.process_rank

#     def next_batch(self):
#         B, T = self.B, self.T
#         buf = self.tokens[self.current_position : self.current_position + B * T + 1]
#         x = (buf[:-1]).view(B, T) # inputs
#         y = (buf[1:]).view(B, T) # targets
#         self.current_position += B * T * self.num_processes

#         if self.current_position + (B * T * self.num_processes + 1) > len(self.tokens):
#             self.current_position = self.B * self.T * self.process_rank

#         return x, y

class CasualSelfAttention(nn.Module):
    def __init__(self):
        super().__init__()
        assert n_embed % n_head == 0, "Embedding dimension must be divisible by number of heads"

        self.n_head = n_head
        self.n_embed = n_embed

        self.c_attention = nn.Linear(n_embed, 3 * n_embed) # for all 3 (key, query, value)
        self.c_proj = nn.Linear(n_embed, n_embed)
        self.c_proj.NANOGPT_SCALE_INIT = 1

    def forward(self, x):
        B, T, C = x.size()
        qkv = self.c_attention(x)
        q, k, v = qkv.split(self.n_embed, dim = 2) # 3 -> (B, T, n_embed)

        head_size = C // self.n_head
        q = q.view(B, T, self.n_head, head_size).transpose(1, 2)
        k = k.view(B, T, self.n_head, head_size).transpose(1, 2)
        v = v.view(B, T, self.n_head, head_size).transpose(1, 2)

        # Flash Attention
        y = F.scaled_dot_product_attention(q, k, v, is_causal = True)

        y = y.transpose(1, 2).contiguous().view(B, T, C) # Reshape back to (B, T, n_embed)
        y = self.c_proj(y)

        return y


class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.input_hidden = nn.Linear(n_embed, 4 * n_embed)
        self.gelu = nn.GELU()
        self.hidden_output = nn.Linear(4 * n_embed, n_embed)
        self.hidden_output.NANOGPT_SCALE_INIT = 1

    def forward(self, x):
        x = self.input_hidden(x)
        x = self.gelu(x)
        x = self.hidden_output(x)
        return x


class Block(nn.Module):
    def __init__(self):
        super().__init__()
        head_size = n_embed // n_head
        self.ln1 = nn.LayerNorm(n_embed)
        self.attention = CasualSelfAttention()
        self.ln2 = nn.LayerNorm(n_embed)
        self.mlp = MLP()

    def forward(self, x):
        x = x + self.attention(self.ln1(x))
        x = x + self.mlp(self.ln2(x))
        return x



class GPT(nn.Module):
    def __init__(self, vocab_size):
        super().__init__()
        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)
        self.position_embedding_table = nn.Embedding(block_size, n_embed)
        self.transformer_blocks = nn.Sequential(*[Block() for _ in range(n_layer)])
        self.final_layer_norm = nn.LayerNorm(n_embed)
        self.language_model_head = nn.Linear(n_embed, vocab_size, bias = False)
        self.language_model_head.weight = self.token_embedding_table.weight
        self.apply(self._init_weights)


    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            std = 0.02
            if hasattr(module, 'NANOGPT_SCALE_INIT'):
                std *= (2 * n_layer) ** -0.5 # 2 -> attention and mlp blocks

            torch.nn.init.normal_(module.weight, mean=0.0, std = std)

            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)

        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)


    def forward(self, idx, targets = None):
        B, T = idx.size()
        assert T <= block_size, f"Sequence length {T} exceeds block size {block_size}"

        token_embeddings = self.token_embedding_table(idx) #(Batch, Time, Channel)
        position_embeddings = self.position_embedding_table(torch.arange(0, T, dtype = torch.long, device = idx.device)) #(Time, Channel)
        x = token_embeddings + position_embeddings

        x = self.transformer_blocks(x)
        x = self.final_layer_norm(x)

        logits = self.language_model_head(x) # (B, T, vocab_size)

        loss = None
        if targets is not None:
            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))

        return logits, loss


    # from github ->
    def configure_optimizers(self, weight_decay, learning_rate, device_type):
        # Get all trainable parameters (that require gradients)
        trainable_params = {name: param for name, param in self.named_parameters() if param.requires_grad}

        # Separate parameters based on weight decay application
        weight_decay_params = [param for name, param in trainable_params.items() if param.dim() >= 2]
        no_weight_decay_params = [param for name, param in trainable_params.items() if param.dim() < 2]

        # Define parameter groups for optimizer
        param_groups = [
            {'params': weight_decay_params, 'weight_decay': weight_decay},
            {'params': no_weight_decay_params, 'weight_decay': 0.0}  # No weight decay for biases and LayerNorm
        ]

        # Count the number of parameters in each group (for debugging)
        num_weight_decay_params = sum(param.numel() for param in weight_decay_params)
        num_no_weight_decay_params = sum(param.numel() for param in no_weight_decay_params)

        if master_process:
            print(f"Weight-decayed parameter tensors: {len(weight_decay_params)}, total parameters: {num_weight_decay_params:,}")
            print(f"Non-weight-decayed parameter tensors: {len(no_weight_decay_params)}, total parameters: {num_no_weight_decay_params:,}")

        # Check if fused AdamW is available for GPU acceleration
        fused_adamw_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters
        use_fused_adamw = fused_adamw_available and device_type == "cuda"

        if master_process:
            print(f"Using fused AdamW: {use_fused_adamw}")

        # Create AdamW optimizer
        optimizer = torch.optim.AdamW(
            param_groups,
            lr = learning_rate,
            betas = (0.9, 0.95),
            eps = 1e-8,
            fused = use_fused_adamw
        )

        return optimizer


    # from github ->
    @torch.no_grad()
    def generate(self, idx, max_new_tokens, temperature = 1.0, do_sample = False, top_k = None):
        for _ in range(max_new_tokens):
            # if the sequence context is growing too long we must crop it at block_size
            idx_cond = idx if idx.size(1) <= block_size else idx[:, -block_size:]
            # forward the model to get the logits for the index in the sequence
            logits, _ = self(idx_cond)
            # pluck the logits at the final step and scale by desired temperature
            logits = logits[:, -1, :] / temperature

            if top_k is not None:
                v, _ = torch.topk(logits, top_k)
                logits[logits < v[:, [-1]]] = -float('Inf')
            # apply softmax to convert logits to (normalized) probabilities
            probs = F.softmax(logits, dim=-1)
            # either sample from the distribution or take the most likely element
            if do_sample:
                idx_next = torch.multinomial(probs, num_samples=1)
            else:
                _, idx_next = torch.topk(probs, k=1, dim=-1)
            # append sampled index to the running sequence and continue
            idx = torch.cat((idx, idx_next), dim=1)

        return idx


model = GPT(vocab_size)
model = model.to(device = device)

############################ Set True in A100 ##################################
use_compile = False
if use_compile:
    model = torch.compile(model)
################################################################################

if ddp:
    model = DDP(model, device_ids = [ddp_local_rank])

if master_process:
    def count_parameters(model):
        return sum(p.numel() for p in model.parameters() if p.requires_grad)

    num_params = count_parameters(model)
    print(f"The model has {num_params} trainable parameters.")

    def count_parameters_by_layer(model):
        total_params = 0
        print("Layer-wise parameter count:")

        for name, param in model.named_parameters():
            if param.requires_grad:
                num_params = param.numel()
                print(f"Layer: {name}, Parameters: {num_params}")
                total_params += num_params

        print(f"Total Trainable Parameters: {total_params}")

    count_parameters_by_layer(model)

drive_base_dir = '/content/drive/MyDrive/GPT_70M_saves/'
log_dir = os.path.join(drive_base_dir, 'checkpoints')
os.makedirs(log_dir, exist_ok = True)

log_file = os.path.join(log_dir, f"log.txt")
if not os.path.exists(log_file): # if not exist, then clear start
    with open(log_file, "w") as f:
        pass


# Remove very old checkpoints to save drive space
import glob

def clean_old_checkpoints(directory, keep = 3):
    ckpts = glob.glob(os.path.join(directory, "checkpoint_step*.pt"))
    ckpts.sort(key = lambda x: int(x.split("checkpoint_step")[1].split(".pt")[0]))
    for ckpt in ckpts[:-keep]:
        os.remove(ckpt)


################## Change for A100(bfloat16) or T4(float16) ####################
torch_dtype = torch.bfloat16
################################################################################

raw_model = model.module if ddp else model
device_type = "cuda" if device.startswith("cuda") else "cpu"

if torch.cuda.is_available():
    torch.cuda.manual_seed(1337)

torch.set_float32_matmul_precision('high')

enc = tiktoken.get_encoding("cl100k_base")

B = batch_size
T = block_size

# In order to simulate GPT-3 0.5M batch size
total_batch_size = 524288 # (2 ^ 19) -> ~0.5M batch size
assert total_batch_size % (B * T * ddp_world_size) == 0, "check total_batch_size portion again"
grad_accumulation_steps = total_batch_size // (B * T * ddp_world_size)

if master_process:
    print(f"Gradient accumulation steps: {grad_accumulation_steps}")
    print(f"=> calculated gradient accumulation steps = {grad_accumulation_steps}")


train_loader = DataLoader(
    B = B,
    T = T,
    process_rank = ddp_rank,
    num_processes = ddp_world_size,
    split = 'train'
  )

val_loader = DataLoader(
    B = B,
    T = T,
    process_rank = ddp_rank,
    num_processes = ddp_world_size,
    split = 'val'
  )


warmup_steps = 222  # calculate by:: max_steps * (715 / 19073)
max_learning_rate = 6e-4
min_learning_rate = 0.1 * max_learning_rate
max_steps = 5912  # 5912 steps -> 3.1B training tokens, change if amount of tokens change

val_interval = 100
generate_interval = 200
checkpoint_interval = 200


# Learning rate cosine variation
def get_learning_rate(it):
    if it < warmup_steps:
        return max_learning_rate * (it + 1) / (warmup_steps)

    if it > max_steps:
        return min_learning_rate

    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)
    assert 0 <= decay_ratio <= 1, f"Decay ratio out of range: {decay_ratio}"

    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0
    return min_learning_rate + coeff * (max_learning_rate - min_learning_rate)


# Optimizer
optimizer = raw_model.configure_optimizers(weight_decay = 0.1, learning_rate = 6e-4, device_type = device_type)


# If resuming training, then load from checkpoint
# checkpoint_path = '/content/drive/MyDrive/GPT_Tokenizer_saves/checkpoints/checkpoint_step1.pt'
checkpoint_path = None

if checkpoint_path:
    if master_process:
        print(f"Resuming from checkpoint: {checkpoint_path}")
        with open(log_file, "a") as f:
            f.write(f"\nResuming from checkpoint: {checkpoint_path}\n")

    checkpoint = torch.load(checkpoint_path, map_location=device)

    raw_model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    step_offset = checkpoint.get('step', 0) + 1

    # Safely load torch RNG state
    torch_rng = checkpoint['torch_rng_state']
    if not isinstance(torch_rng, torch.ByteTensor):
        torch_rng = torch_rng.cpu().clone().detach().type(torch.ByteTensor)
    torch.set_rng_state(torch_rng)

    # Safely load CUDA RNG states
    if device_type == "cuda":
        cuda_rng_state = checkpoint['cuda_rng_state']
        cuda_rng_state = [
            s.cpu().clone().detach().type(torch.ByteTensor)
            if not isinstance(s, torch.ByteTensor) else s.cpu()
            for s in cuda_rng_state
        ]
        torch.cuda.set_rng_state_all(cuda_rng_state)

else:
    step_offset = 0
    if master_process:
        print("Starting fresh training loop")
        with open(log_file, "a") as f:
            f.write("\nStarting fresh training loop\n")


time_tracker = []

# Training loop
for step in range(step_offset, max_steps):
    last_step = (step == max_steps - 1)

    # once in a while evaluate our validation loss
    if step % val_interval == 0 or last_step:
        time_validation_loss_start = time.time() ;

        model.eval()
        val_loader.reset()

        with torch.no_grad():
            val_loss_accum = 0.0
            val_loss_steps = 310

            for _ in range(val_loss_steps):
                x, y = val_loader.next_batch()
                x, y = x.to(device), y.to(device)

                with torch.autocast(device_type = device_type, dtype = torch_dtype):
                    logits, loss = model(x, y)

                loss = loss / val_loss_steps
                val_loss_accum += loss.detach()

        if ddp:
            dist.all_reduce(val_loss_accum, op = dist.ReduceOp.AVG)

        if master_process:
            print(f"validation loss: {val_loss_accum.item():.4f}")
            with open(log_file, "a") as f:
                f.write(f"{step} validation loss = {val_loss_accum.item():.4f}\n")

            time_validation_loss = time.time() - time_validation_loss_start ;
            print("Validation loss time =", time_validation_loss)

            time_checkpointing_start = time.time()

            if step > 0 and (step % checkpoint_interval == 0 or last_step):
                checkpoint = {
                    'model_state_dict': raw_model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'step': step,
                    'val_loss': val_loss_accum.item(),
                    'torch_rng_state': torch.get_rng_state().cpu().clone().detach().type(torch.uint8),
                    'cuda_rng_state': [s.clone().detach().cpu().type(torch.uint8) for s in torch.cuda.get_rng_state_all()],
                    'grad_accumulation_steps': grad_accumulation_steps,
                    'batch_size': batch_size,
                    'block_size': block_size,
                    'learning_rate': learning_rate,
                    'max_steps': max_steps,
                    'manual_lr_schedule': {
                        'warmup_steps': warmup_steps,
                        'max_lr': max_learning_rate,
                        'min_lr': min_learning_rate
                    }
                }

                torch.save(checkpoint, os.path.join(log_dir, f"checkpoint_step{step}.pt"))
                clean_old_checkpoints(log_dir)

                time_checkpointing = time.time() - time_checkpointing_start
                print("Checkpointing time =", time_checkpointing)



    # # once in a while generate from the model (except step 0, which is noise)
    if ((step > 0 and step % generate_interval == 0) or last_step) and (not use_compile):
        model.eval()
        num_return_sequences = 4
        max_length = 200

        tokens = enc.encode("while(True):")
        tokens = torch.tensor(tokens, dtype = torch.long)
        tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)
        xgen = tokens.to(device)
        sample_rng = torch.Generator(device = device)
        sample_rng.manual_seed(42 + ddp_rank)

        while xgen.size(1) < max_length:
            with torch.no_grad():
                with torch.autocast(device_type = device_type, dtype = torch_dtype):
                    logits, loss = model(xgen) # (B, T, vocab_size)

                logits = logits[:, -1, :] # (B, vocab_size)
                probs = F.softmax(logits, dim = -1)
                topk_probs, topk_indices = torch.topk(probs, 50, dim = -1)
                ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)
                xcol = torch.gather(topk_indices, -1, ix) # (B, 1)
                xgen = torch.cat((xgen, xcol), dim=1)

        for i in range(num_return_sequences):
            tokens = xgen[i, :max_length].tolist()
            decoded = enc.decode(tokens)
            print(f"rank {ddp_rank} sample {i}: {decoded}")


    time_training_start = time.time()

    model.train()
    optimizer.zero_grad(set_to_none = True)
    loss_accum = 0.0

    for micro_step in range(grad_accumulation_steps): # to simulate large parallel batch size in gpt(0.5 Million)
        x_batch, y_batch = train_loader.next_batch()
        x_batch, y_batch = x_batch.to(device), y_batch.to(device)

        if ddp:
            model.require_backward_grad_sync = (micro_step == grad_accumulation_steps - 1)

        with torch.autocast(device_type="cuda", dtype = torch_dtype):
            logits, loss = model(x_batch, y_batch)

        loss = loss / grad_accumulation_steps
        loss_accum += loss.detach()
        loss.backward()

    if ddp:
        dist.all_reduce(loss_accum, op = dist.ReduceOp.AVG)

    # gradient clipping
    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

    learning_rate = get_learning_rate(step)

    for param_group in optimizer.param_groups:
        param_group['lr'] = learning_rate

    optimizer.step()
    if device_type == "cuda":
        torch.cuda.synchronize()

    time_training = time.time() - time_training_start # time difference in seconds
    tokens_processed = train_loader.B * train_loader.T * grad_accumulation_steps * ddp_world_size
    tokens_per_sec = tokens_processed // time_training


    time_tracker.append(time_training)
    if len(time_tracker) > 10:
        time_tracker.pop(0)  # keep last 10 steps

    avg_dt = sum(time_tracker) / len(time_tracker)
    eta_min = avg_dt * (max_steps - step - 1) / 60
    eta_hours = int(eta_min // 60)
    eta_minutes = int(eta_min % 60)

    if master_process:
        log_string = (f"Step {step}, Loss: {loss_accum.item():.6f} | LR = {learning_rate:.4e} | "
                  f"GradNorm = {norm:.4f} | Time: {time_training:.2f}s | tok/sec = {tokens_per_sec} | ETA: {eta_hours}hrs {eta_minutes}min")

        print(log_string, flush = True)

        with open(log_file, "a") as f:
            f.write(log_string + "\n")


if ddp:
    destroy_process_group()


if master_process: #
    print("Training finished. Saving final model checkpoint...")
    final_checkpoint_path = os.path.join(log_dir, "final_model.pt")

    final_checkpoint = {
        'model_state_dict': raw_model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'step': step,
        'val_loss': val_loss_accum.item(),
        'torch_rng_state': torch.get_rng_state().cpu().clone().detach().type(torch.uint8),
        'cuda_rng_state': [s.clone().detach().cpu().type(torch.uint8) for s in torch.cuda.get_rng_state_all()],
        'grad_accumulation_steps': grad_accumulation_steps,
        'batch_size': batch_size,
        'block_size': block_size,
        'learning_rate': learning_rate,
        'max_steps': max_steps,
        'manual_lr_schedule': {
            'warmup_steps': warmup_steps,
            'max_lr': max_learning_rate,
            'min_lr': min_learning_rate
        }
    }
    torch.save(final_checkpoint, final_checkpoint_path)
    print(f"Final model saved to: {final_checkpoint_path}")



